Если не уверены, что справитесь с первого раза, то нужно сделать бэкапы, заходим в VMware под своим адресом, 
нажимаете на машину и сделать снимок состояния. 
Теперь при любом косяке можно вернутся в начало либо делать снимки при достижении какого-либо этапа.
Первым делом заходим в нетворги и создаем группы портов столько сколько нужно по заданию
INT
HQ
BR
Далее заходим в каждую машину и по топологии добавляем столько портов сколько нужно, оставляя 1 как выход в инет,
и так с каждой машиной.

Теперь пока все не испорчено или правильно сделано, скачаем все что нужно для выполнения заданий в дальнейшем:
Apt install frr(для frr, на всех роутерах(HQ, BR, ISP)
Apt install isc-dhcp-server(для dhcp, только на HQ)
Apt install radvd(настройка маршрутизации)
Apt install iperf3(для измерения пропускной способности на 2х маршрутизаторах HQ и ISP)
Apt install bind9 dnsutils(пакеты для днс будет стоять на сервере)(не надо)
apt install iptables-persistent(SRV для ssh)

Если закачка не идет то нужно вводить эти команды пока не заработает:
Dhclient -r
Dhclient -v
Далее заходим на каждую машину и переименовываем их, там же вводим адрес в соответствии с топологией.
Все можно ввести, введя команду nmtui, если его нет вводим команду apt install network-manager,
в Set system hostname вводим имя, в Edit a connection вводим адреса, 
и вторая вкладка перезапускаем чтобы настройки принялись.
HQ - 192.168.2.0/29 255.255.255.248 - 2001::2:0/125 SRV.2 HQ-R.1
HQ-ISP - 192.168.10.0/30 255.255.255.252 - 2001::10:0/126 HQ-R.1 ISP.2
INT - 192.168.1.0/24 255.255.255.0 - 2001::1:0/120 CLI.2 ISP.1
ISP-BR - 192.168.10.0/30 255.255.255.252 - 2001::10:0/126 BR-R.6 ISP.5
BR - 192.168.3.0/27 - 255.255.255.240 - 2001::3:0/123 SRV.2 BR-R.1
HQ-R
192.168.2.1/29
2001::2:1/125
192.168.10.1/30
2001::10:1/126
HQ-SRV
192.168.2.2/29
192.168.2.1
2001::2:2/125
2001::2:1
BR-R
192.168.3.1/27
2001::3:1/123
192.168.10.1/30
2001::10:1/126
BR-SRV
192.168.3.2/27
192.168.3.1
2001::3:2/123
2001::3:2
ISP
192.168.10.2/30
2001::10:2/126
192.168.1.1/24
2001::1:1/120
192.168.10.5/30
2001::10:5/126
CLI
192.168.1.2/24
192.168.1.1
2001::1:2/120
2001::1:1
После ввода имени нужно прописать команду newgrp это нужно для обновления, чтобы имя встало сразу.

Настройка FRR
Nano /etc/frr/daemons
ospfd=yes
ospf6d=yes
Systemctl restart frr
Vtysh
Conf t
Router ospf
Ospf router-id 1.1.1.1|2.2.2.2|3.3.3.3
HQ – Network 192.168.10.3/30 area 0  
HQ – Network 192.168.2.0/29 area 1 
ISP – network 192.168.10.3/30 area 0 
ISP – network 192.168.1.0/24 area 2 
ISP – network 192.168.10.3/30 area 0 
BR – network 192.168.10.3/30 area 0
BR – network 192.168.3.0/27 area 3
Ex
Ex
Write
Ex
Nano /etc/sysctl.conf
переменную net.ipv4.ip_forward=1 необходимо раскоментить и сохранить изменения в файле, 
и применить изменения командой sysctl -p
Так же и с 6 net.ipv6.conf.all.forwarding=1
 
Router ospf6
Ospf6 router-id 0.0.0.1|0.0.0.2|0.0.0.3
HQ – Area 0.0.0.0 range 2001::10:0/126
HQ – Area 0.0.0.0 range 2001::2:0/125
ISP – area 0.0.0.0 range 2001::10:0/126
ISP – area 0.0.0.0 range 2001::1:0/120
BR – area 0.0.0.0 range 2001::10:0/126
BR – area 0.0.0.0 range 2001::3:0/123
Ex
Ex
Write
Ex

Нужно еще сделать один момент для работы ipv6:
Прописываем interface ens224*
	ipv6 ospf6 area 0.0.0.0
	ex
	!
	interface ens256*
И так на каждый интерфейс.
		
Настройка DHCP
nano /etc/default/isc-dhcp-server
Interfacesv4="ens* ens*"
Interfacesv6="ens* ens*"

nano /etc/dhcp/dhcpd.conf
default-lease-time 600;
max-lease-time 7200;
#ddns-updates on;
#ddns-update-style interim;
autoritative;

nano /etc/default/isc-dhcp-server
раскомментить все v4 и если по задание необходимо и v6
изменить так же нужно: DHCPv4_PID=/var/run/dhclient.ens192.pid(192 если этот int интернет)

subnet 192.168.2.0 netmask 255.255.255.248 {
 range 192.168.2.3 192.168.2.6;
 option routers 192.168.2.1;
# option domain-name "hq.work";
# option domain-name-servers 192.168.2.2;
}

после каждого изменения конфигурации необходимо перезагружать DHCP сервер для применения конфигурации
systemctl stop isc-dhcp-server
systemctl start isc-dhcp-server
А для того, чтобы после перезагрузки DHCP-сервер автоматически включался можно воспользоваться командой 
systemctl enable isc-dhcp-server
v6
nano /etc/dhcp/dhcpd6.conf
default-lease-time 2592000;
preferred-lifetime 604800;
option dhcp-renewal-time 3600;
option dhcp-rebinding-time 7200;
allow leasequery;

subnet6 2001::2:0/125 {
	range6 2001::2:3 2001::2:3e;
option dhcp6.name-servers 2001::2:2;
option dhcp6.domain-search "hq.work";
}
option dhcp6.info-refresh-time 21600;
autoritative;


radvd
После установки нужно сконфигурировать файл по пути /etc/radvd.conf следующего содержания
interface ens* (в сторону сервера, клиента)
{
MinRtrAdvInterval 3;
MaxRtrAdvInterval 60;
AdvSendAdvert on;
};
После окончания конфигурирования так же необходимо перезагрузить службу Radvd и отправить в Enable
systemctl stop radvd
systemctl start radvd
systemctl enable radvd

Учетки
adduser admin/branch/network
P@ssword
Admin/Branch admin/Network admin
Так же возможно понадобится выдать Root права для данных клиентов это можно выполнить посредством команды visudo
admin   ALL=(ALL:ALL) ALL (CLI HQ-SRV HQ-R)
branch  ALL=(ALL:ALL) ALL (BR-SRV BR-R)
network ALL=(ALL:ALL) ALL (HQ-R BR-R BR-SRV)

IPERF
После установки на обоих машинах, достаточно воспользоваться командной
iperf3 -c (ip адрес проверяемой машины) -i1 -t20

backup
Для начала на машинах HQ-R, BR-R создадим каталог, где будет хранится файл созданного скриптом бекапа.
Можно создать его в директории mnt
для этого пропишем mkdir /mnt/backup
Далее нам нужно создать сам файл для создания бэкап скрипта, для этого пропишем команду
touch /etc/backup.sh

#!/bin/bash
backup_files="/home /etc /root /boot /opt"
dest="/mnt/backup"
arhive_file="backup.tgz"
echo "Backing up $backup_files to $dest/$archive_file"
tar czf $dest/$archive_file $backup_files
echo "backup finished"
LS -lh $dest

где backup_files — копируемые директории
dest — место куда копируем директории
day — параметр который указывает день бэкапа
hostname — имя от кого он выполнился
archive_file — конечное имя файла
tar czf — в месте указанное в dest помещает файл с именем указанным в archive_file с содержимым указанным
в backup_files
echo — необязательные строки вывода
Для запуска скрипта достаточно написать bash (имя_файла)
После создания скрипта для того, чтобы распаковать наш backup архив можно воспользоваться командой
tar -xvpzf /mnt/backup/backup.tgz -C / --numeric-owner
Для того что бы не писать скрипт дважды, можно c помощью ssh перекинуть его на вторую машину посредством команды scp 
для начала подключаемся по ssh командой ssh имя@адрес
Пример: ssh network_admin@192.168.1.1
затем посредством команды 
scp /расположение/имя_файла имя@адрес :/расположение/имя_файла
Пример:
 scp /etc/backup.sh network_admin@192.168.2.1:/home/network_admin
После успешного копирования возвращаемся в нашу машину и можем перенести скрипт в любое более удобное место
После всего нужно запустить скрипт: bash (имя_файла) 
		
SSH
Первым делом необходимо перейти по пути nano /etc/ssh/sshd_config где в окне конфигурации нам необходимо на HQ-SRV
найти строку и изменить значения
Port 2222
Для применения конфигурации необходимо перезагрузить службу командой systemctl restart ssh
Для перенаправления трафика воспользуемся утилитой iptables-persistent
которая устанавливается командой apt install iptables-persistent(в начале качаем)
После установки создадим правило на подмену порта командой
iptables -t nat -A PREROUTING -d 192.168.2.0/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 192.168.2.2:2222
Для того что бы не прописывать команду при каждой перезагрузке сохраним нашу текущую конфигурацию командой 
iptables-save > /etc/iptables/rules.v4 

Контроль доступа

В зависимости от учётной записи, которая должна иметь доступ до сервера возможны следующие развития события, 
если нам необходим доступ только от локальных учётных записей,
то шаг 1 после всех настроек необходимо вернуть в исходный вид
Шаг 1 
Заходим в настройки ssh по пути использованному ранее
nano /etc/ssh/sshd_config
находим и меняем строку
 PermitRootLogin yes 

после сохранения изменений перезагружаем службу ssh
Шаг 2
Следующим шагом необходимо создать ключ аутентификации ssh  с помощью команды 
ssh-keygen -С «имя_устройства_с_которого_создан_ключ» везде необходимо нажать ENTER пока не создастся ключ 
Теперь необходимо перенести публичный ключ на сервер к которому мы будем получать доступ с помощью команды 
ssh-copy-id имя@адрес
Пример: 
ssh-copy-id root@192.168.1.2
	ssh-copy-id admin@192.168.1.2
	Последним шагом запретим любой доступ клиенту до нашего сервера
На HQ-SRV переходим по пути
nano /etc/hosts.deny
и вносим следующую строку в файл
sshd: 192.168.1.2 (адрес машины CLI)
перезагружаем ssh
В конце не забудьте отключить доступ по root, если иного не указано в задании !

Модуль 2.
Вся настройка будет происходить на сервере HQ-SRV
Первым делом необходимо установить пакеты для dns командой
apt install bind9 dnsutils(в начале можно было установить или уже сделали)
Следующим шагом необходимо создать зоны для прямого и обратного просмотра dns
Для этого переходим по пути nano /etc/bind/named.conf.default-zones и создаём зоны как показано на скриншотах ниже
zone "hq.work" {
	type master;
	file "/etc/bind/hq";
	allow-update {any;};
	allow-transer {any;};
};

zone "2.168.192.in-addr.arpa" {
	type master;
	file "/etc/bind/hq_arpa";
	allow-update {any;};
};

zone "0.0.0.2.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.2.ip6.arpa" {
	type master;
	file "/etc/bind/hq6_arpa";
	allow-update {any;};
};
br.work
zone "br.work" {
	type master;
	file "/etc/bind/br";
	allow-update {any;};
	allow-transer {any;};
};

zone "3.168.192.in-addr.arpa" {
	type master;
	file "/etc/bind/br_arpa";
	allow-update {any;};
};

zone "0.0.0.3.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.2.ip6.arpa" {
	type master;
	file "/etc/bind/br6_arpa";
	allow-update {any;};
};

Следующим шагом необходимо создать конфигурационные файлы для наших зон. Это можно сделать,
скопировав стандартные шаблоны командой cp
Пример:
cp /etc/bind/db.local /etc/bind/hq — создание файла для прямой зоны
cp /etc/bind/db.127 /etc/bind/hq_arpa — создание обратной зоны ipv4
Зону для ipv6 скопируем после конфигурации зоны для ipv4 (так как по содержанию они не отличаются)
Первым шагом сконфигурируем зону прямого просмотра, переходим по пути
nano /etc/bind/hq 
И там заполняем по картинке 1(днс)
}
} BIND data file for local loopback interface
}
$TIL 694800
@	 IN	SOA	HQ.work. root.hq.work. (
				2	; Serial
			   604800	; Refresh
			    86400	; Retry
			  2419200	; Expire
			   604800 )	; Negative Cache TTL 
;
@	IN	NS	hq.work.
@	IN	A	192.168.2.2
HQ-SRV	IN	AAAA	2001::2:2
HQ-SRV	IN	A	192.168.2.2
HQ-R	IN	A	192.168.2.2
HQ-R	IN	AAAA	2001::2:1
SERVER	IN	CNAME	HQ-SRV


Вторым шагом настроим зону обратного просмотра как указано на скриншоте ниже
Зона находится по пути 
nano /etc/bind/hq_arpa
И там заполняем по картинке 2(днс)

;
; BIND data file for local loopback interface
;
$TIL 604800
@	 IN	SOA	HQ.work. root.hq.work. (
				1	; Serial
			   604800	; Refresh
			    86400	; Retry
			  2419200	; Expire
			   604800 )	; Negative Cache TTL 
;
@	IN	NS	hq.work.
2	IN	PTR	HQ-SRV.hq.work.
1	IN	PTR	HQ-R.hq.work

Третьим шагом настроим запись для зоны обратного просмотра для ipv6, для этого достаточно скопировать зону hq_arpa, 
то есть
cp /etc/bind/hq_arpa /etc/bind/hq6_arpa
После создания всех конфигов необходимо перезагрузить службу bind9
systemctl restart bind9 (лучше stop и start)
Похожая настройка выполняется для зоны branch.work
Проверка выполняется посредством команд
host IP-адрес
host имя машины
Примечание:
Не забывайте, что для br-srv по заданию нет PTR записи, её создание может считаться ошибкой

Время.
Настройка производится на всех машинах, указанных в топологии, при этом настройка на машине, 
выступающей в роли NTP сервера уникальна, а на NTP клиентах идентична
Для начала на всех машинах необходимо установить московский часовой пояс, для этого следует воспользоваться командой 
timedatectl set-timezone Europe/Moscow
Следующим шагом установим альтернативную службу NTP, под названием CHRONY, так как для задания 3, 
где происходит развёртывание домена, будет использоваться именно этот сервис. Устанавливаем с помощью команды:
apt install chrony
Произведём установку NTP сервиса Chrony
Далее следует осуществить настройку машины, выступающей в роли NTP сервера HQ-R, посредством команды
nano /etc/chrony/chrony.conf
осуществим вход в конфигурацию chrony, где следует установить значения 
local stratum 5
allow 192.168.10.0/8
allow 192.168.1.0/8
Настройка производится на всех машинах, указанных в топологии, при этом настройка на машине,
выступающей в роли NTP сервера уникальна, а на NTP клиентах идентична
Для начала на всех машинах необходимо установить московский часовой пояс, для этого следует воспользоваться командой 
timedatectl set-timezone Europe/Moscow
Следующим шагом установим альтернативную службу NTP, под названием CHRONY, так как для задания 3,
где происходит развёртывание домена, будет использоваться именно этот сервис. Устанавливаем с помощью команды:
apt install chrony
Произведём установку NTP сервиса Chrony
Далее следует осуществить настройку машины, выступающей в роли NTP сервера HQ-R, посредством команды
nano /etc/chrony/chrony.conf
осуществим вход в конфигурацию chrony
server 192.168.2.1
Для проверки используйте команды chronyc tracking и chronyc sources

Сервер домена.
ДЛЯ настройки будет выбрана именно FreeIpa.
Первым делом необходимо установить докер, воспользовавшись скриптом, который есть в открытом доступе, 
однако для этого нам необходимо экспортировать переменные окружения относящиеся к Proxy 
(Если Proxy отсутствует т. е. Пакеты с не стандартных репозиториев устанавливаются сами,
то первый шаг можно пропустить)
Первым шагом необходимо посмотреть переменные, которые необходимо экспортировать, перейдя по пути
nano /etc/apt/apt.conf.d/01proxy
и посмотреть находящиеся там значения, после чего посредством команд
export http_proxy=http(или https)://(адрес:порт)
export https_proxy=http(или https)://(адрес:порт)
 Экспортировать переменные прокси для доступа в интернет 
Acquire::http::Proxy "http://10.0.70.52:3128";
Acquire::https::Proxy "http://10.0.70.52:3128";
export http_proxy=http://10.0.70.52:3128
export https_proxy=http://10.0.70.52:3128
Вторым шагом посредством скрипта необходимо установить сам DOCKER, для этого необходимо ввести следующую команду
wget -qO- https://get.docker.com | bash 
Вся установка происходит автоматически, и не должна выдавать ошибок, если были выполнены все предыдущие шаги
Третьим шагом необходимо запулить готовый контейнер с образом freeipa для centos-8-4.8.4 
Для этого создаём каталог для автоматического запуска служб докера (Необходимо если вы делали шаги с Proxy ранее),
командой
mkdir -p /etc/systemd/system/docker.service.d
Далее заходим в файл 
nano /etc/systemd/system/docker.service.d/http-proxy.conf 
[Service]
Environment="HTTP_PROXY=http://10.0.70.52:3128"
Environment="HTTPS_PROXY=http://10.0.70.52:3128"
После чего перезапускаем демона и сам докер командами в указанном порядке
systemctl daemon-reload
и
systemctl restart docker
После чего запускаем команду
docker pull freeipa/freeipa-server:centos-8-4.8.4
После окончания пула контейнера необходимо создать директорию,
 в которую будет монтироваться контейнер посредством команды
mkdir -p /var/lib/ipa-data
 Также необходимо внести изменения в загрузчик системы для указания, 
необходимости использования обоих версий cgroup (механизм по ограничению ресурсов,
начиная с 11 Debian по умолчанию включена только 2 версия)
Для этого посредством команды заходим в загрузчик ядра
nano /etc/default/grub 
GRUB_CMDLINE_LINUX="quiet systemd.unified_cgroup_hierarchy=0"
Для применения изменений необходимо использовать команду
grub-mkconfig -o /boot/grub/grub.cfg
После чего необходимо перезагрузить машину
Следующим шагом уже переходим к запуску контейнера с хранящейся там FreeIPA,
в качестве параметров ключей, указывает имя, указываем доменную сеть,
а так открываем все необходимые для работы порты, указываем путь и образ, разрешаем конфликт с IPv6.
docker run --name freeipa-server -ti -h hq-srv.work -p 80:80 -p 443:443 -p 389:389 -p 636:636 -p 88:88 -p 464:464
-p 88:88/udp -p 464:464/udp -p 123:123/udp --read-only --sysctl net.ipv6.conf.all.disable_ipv6=0 -v 
/sys/fs/cgroup:/sys/fs/cgroup:rw -v /var/lib/ipa-data:/data:Z freeipa/freeipa-server:centos-8-4.8.4
Важное Примечание: В случае завершения выполняемых функций в контейнере в результате которых оболочка может перейти в состояние freezing, или при успешном завершении, для выхода из оболочки окружения необходимо последовательно нажать сочетание клавиш ctrl + p, а затем ctrl + q. В случае если вам необходимо остановить контейнер можно воспользоваться командой docker stop имя контейнера, для удаления контейнера docker rm имя контейнера, для просмотра существующих образов docker images 
После успешного запуска необходимо заполнить форму:
На вопрос о интеграции DNS нажимаем Enter
На вопрос о задании имени сервера нажимаем Enter
На вопрос о подтверждение имени домена нажимаем Enter
На вопрос о подтверждение имени области нажимаем Enter
На запрос ввода пароля для менеджера директорий вводим P@ssw0rd
На запрос ввода пароля для IPA админа вводим P@ssw0rd
На вопрос синхронизации с службой Chrony нажимаем Enter
На вопрос о конфигурирование системы с текущими параметрами вводим yes
Процесс установки достаточно длительный и может занимать около 5-10 или более минут.
После завершения установки необходимо подготовить машины, которые будут присоединены к домену. Для этого первым делом переходим по пути:
Nano /etc/hosts
127.0.0.1	localhost
127.0.1.1	cli.hq.work	cli

192.168.2.2 hq-srv.hq.work (hq.srv.serv)

Для машины BR-SRV настройка будет выглядеть 
127.0.0.1	localhost
127.0.1.1	br-srv.branch.work	br-srv

192.168.2.2 hq-srv.hq.work (hq.srv.serv)

Следующим шагом посредством команды:
apt install freeipa-client
Производим установку клиентской части FreeIPA для ввода машины в домен.
На все всплывающие окна во время установки нажимаем Enter
После установки клиента, для ввода машины в домен необходимо прописать команды: 
НА CLI
ipa-client-install --mkhomedir --domain hq.work --server=hq-srv.hq.work -p admin -W
НА BR-SRV
ipa-client-install --mkhomedir --domain branch.work --server=hq-srv.hq.work -p admin -W

На сообщение о продолжении с фиксированными значения пишем yes
На вопрос о конфигурирование CHRONY нажимаем ENTER
На вопрос о конфигурировании с текущими значение пишем yes
Для проверки входа в FreeIPA, на клиентской машине необходимо открыть браузер и в адресной строке написать IP адрес 
машины HQ-SRV (192.168.2.2) логин и пароль для входа в вебку FreeIPA: admin и P@ssw0rd
Важное Примечание: если вы перезагрузите машину, то контейнер выключится, для его запуска можно воспользоваться 
командой  docker start freeipa-server

SMB или NFS
Исходя из поставленной задачи NFS будет более удачным выбором из-за его большей совместимости с системами Linux, 
при этом SMB крайне пере-гружена за счёт того, что создан для совместного использования широкого спектра сетевых 
ресурсов, включая службы файлов и печати, устройства хра-нения данных и хранилища виртуальных машин, в время как NFS, 
для сов-местного использования файлов и каталогов.
Поскольку файловый сервер будет работать на основе NFS , первым де-лом необходимо установить NFS сервер , 
посредством команды:
apt install nfs-kernel-server
Далее необходимо создать каталоги которые будут расшариваться.
mkdir /mnt/all — создание корневого каталога в котором будут хранит-ся остальные
mkdir /mnt/all/Branch_Files — каталог для пользователя branch_admin
mkdir /mnt/all/Network — каталог для пользователя network_admin
mkdir /mnt/all/Admin_Files — каталог для пользователя admin
Так же для того что бы монтируемые директории не были пустыми , и был виден результат монтирования посредством команд
touch /mnt/all/Branch_Files/123
touch /mnt/all/Network/234
touch /mnt/all/Admin_Files/345
Создадим файлы с разными имена в директориях
Далее посредством команды 
nano /etc/exports 
Заходим в конфигурационный файл , где будут прописываться все об-щие ресурсы и их параметры и заполняем
/mnt/all/Branch_Files *(rw,async,no_subtree_check)
/mnt/all/Network_Files *(rw,async,no_subtree_check)
/mnt/all/Admin_Files *(rw,async,no_subtree_check)
где:
 /mnt/all/имя — Указание директории на сервере до которой будет вы-дан общий доступ
* - указание IP адресов, которые имеют доступ в эту директорию (звёз-дочка значит все, 
так как по заданию не указано делать ограничения)
rw — разрешение на чтение и запись
async — включение обработки запросов клиента , до окончания преды-дущего действия
no_subtree_check — отключает проверку вложенных директорий
Для экспорта всех общих ресурсов необходимо воспользоваться коман-дой
exportfs -ra 
Также ещё одним необходимым шагом является создание доменных пользователей в Freeipa домене,
для этого посредством адреса необходимо зайти  в web-интерфейс Freeipa (адрес 192.168.2.2) , 
и сконфигурировать всех пользователей которые необходимы по заданию
Важное примечание: Пользователь admin является встроенной учётной записью и его конфигурировать не нужно.
Во вкладке users необходимо нажать кнопку add
И в появившемся окне необходимо заполнить следующие данные:
user login — network_admin или branch_admin
first name — Network Admin или Branch Admin
last name - Network Admin или Branch Admin
New Password - 123
Verify Password -123
Пароль задаётся 123 , поскольку после захода в систему, необходимо будет сменить пароль
После ввода параметров необходимо нажать Add and Edit
и сконфигурировать параметры Login shell и Home directory. Пример конфигурирования для пользователя branch_admin
где:
Login shell — изменения оболочки окружения в которую будем попа-дать при входе с sh(shell) на bash
Home directory — изменение домашней директории , необходимо так как иначе она будет совпадать с директориями локальных пользователей со-зданных на машинах
После этого можно перейти к настройке клиента , т. к. в задании указано что монтирование должно осуществляться 
при входе доменного пользователя , настройка будет проводится на машинах которые занесены в домен CLI и BR-SRV
Первым шагом необходимо установить NFS-клиент и Pam модуль для автоматического монтирования разделов при входе 
пользователя командой:
apt install nfs-common libpam-mount
Так же необходимо создать каталог куда будет проводится монтирова-ние
mkdir /mnt/all
После чего перейдя по пути
nano /etc/security/pam_mount.conf.xml
Необходимо добавить строки приведённые ниже в разделе <volume definitions>
<volume user="admin" fstype="nfs" server="192.168.2.2" path="/mnt/all/Admin_Files" mountpoint="/mnt/all" />
<volume user="branch_admin" fstype="nfs" server="192.168.2.2" path="/mnt/all/Branch_Files" mountpoint="/mnt/all" />
<volume user="network" fstype="nfs" server="192.168.2.2" path="/mnt/all/Network" mountpoint="/mnt/all" />
Для проверки работы общих ресурсов необходимо зайти под доменным пользователем, для этого посредством команды 
sudo login
переходим в окно для входа в систему
в владке Login указывается пользователь по шаблону :
имя@домен
Пример:
branch_admin@hq.work
В вкладке Password вводится пароль, для пользователей branch_admin и network_admin необходимо будет ввести пароль по 
схеме:
Password: 123
Current Password: 123
New password: P@ssw0rd
Retype new password: P@ssw0rd
И зайдя в пользователя проверить содержимое папки /mnt/all на наличие созданных файлов

веб-сервер LMS.
Вся настройка пунктов A и B будет выполнятся исключительно на ма-шине BR-SRV, для пункта C,
а также проверки пункта A необходимо вос-пользоваться машиной CLI, так как на ней присутствует графика.
Первым шагом необходимо установить пакеты для веб-сервера APACHE и пакеты поддержки PHP, так как PHP, 
быстрее всего позволит со-здать страницу с номер места сдающего.
Для этого посредством команды
Apt install apache2 libapache2-mod-php
Устанавливаются пакеты для apache сервера и поддержки PHP сервером
Далее необходимо сконфигурировать страницу, которой в будущем за-менится дефолтная страница APACHE, командой
nano /var/www/html/mesto.php
Создаётся страница, которую необходимо заполнить
<?php
$fontSize = "200px";
echo "<div style=\"text-align:center\">";
print '<p style="font-size:' .htmlspecialchars($fontSize). '; ">5/p>';
?>

Единственная часть  кода, которую необходимо будет менять, это цифра 5 ,
её будет необходимо заменить на номер своего места.

Следующим шагом необходимо заменить дефолтную страницу, 
для того что бы при обращение к серверу в качестве главной страницы, показывался номер места,
для этого перейдя по пути
nano /etc/apache2/apache2.conf
Переходим в конфигурационный файл, и ищем и заполняем раздел
 <Directory /var/www/>
	options Indexes FollowSymLinks
	AllowOverride None
	Require all granted
	DirectoryIndex mesto.php
</Directory>
Для проверки достаточно зайти на машину CLI, и в браузере прописать IP-адрес сервера, если ошибок допущено не было,
должна быть выведена цифра по центру.
Далее переходим к установке базы данных , т. к. напрямую mysql-server установить не получится,
будет использоваться пользовательский пакет, необ-ходимо снова прописать команду для экспорта переменных которая была 
в  задание 3.
Далее установим один из пакетов необходимых для работы mysql ко-мандой
apt install gnupg
Далее посредством команды
wget https://dev.mysql.com/get/mysql-apt-config_0.8.29-1_all.deb
Указывается путь откуда будет скачиваться пакет, после чего для уста-новки не user friendly пакетов,
используется команда
dpkg -i ./mysql-apt-config_0.8.29-1_all.deb
После чего при установке в появившемся окне просто выбирается вариант OK
Далее для обновления репозитория mysql необходимо прописать:
apt update 
После успешного обновления, можно переходить к установке пакетов для mysql , для этого командой
apt install mysql-server php-mysql
Устанавливаются пакеты сервера, и его совместимости с php, второй из них пригодится чуть позже.
Во время установки будет предложено установить пароль, указывается пароль P@ssw0rd, на вопрос о плагине аутентификации 
выбирается первый вариант
Далее для создания пользователей и другим ,можно установить веб-интерфейс для субд mysql, под названием phpmyadmin, 
этот шаг не обязате-лен, если вы самостоятельно можете создать пользователей и группы че-рез консоль управления 
mysql-server.
Для установки phpmyadmin, необходимо воспользоваться командой:
apt install phpmyadmin
Во время установки:
На вопрос о выборе сервера для конфигурации нажимаем Space (Пробел) напротив Apache2, что бы появилась звёздочка.
После чего Enter.
На вопрос о конфигурирование БД для phpmyadmin выбирается вари-ант YES
Во всех вариантах где необходимо ввести пароль вводится пароль P@ssw0rd
Далее необходимо перейти по адресу
IP-адрес сервера/phpmyadmin
Пример
192.168.2.2/phpmyadmin
В окне авторизации:
В поле Username вводится root (регистр имеет значение)
В поле Password вводится P@ssw0rd
После успешной авторизации заходим в User account(add user account)
В открывшемся окне, заполняются поля(имя, пароль) и go снизу
После создания снова необходимо перейти в вкладку User accounts,
В открывшемся окне указывается имя группы которая будет создана.
Потом заходим там же в user account overview, edit user group.
И в открывшемся окне, указывается группа

Mediawiki.
Первым шагом необходимо установить docker compose, так как сам до-кер устанавливался в задании №3 второго модуля.
Для этого посредством команды и скачиваем:
curl -L "https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-$(uname -$)-$(uname -m) 
-o /usr/local/bin/docker-compose
Выдача прав: chmode +X /usr/local/bin/docker-compose
Далее для того, чтобы с нуля не писать yml файл, можно скачать похо-жий по смыслу файл, 
приведённый на рисунке ниже (если будет запрещено, будете писать сами)
wget -L "https://raw.githubusercontent.com/pirate/wikipedia-mirror/master/docker-compose.mediawiki.yml" 
-O /home/admin/wiki.yml
После чего открываем скачанный файл по пути
Nano /home/admin/wiki.yml
И приводим к виду, указанному на рисунке ниже, не удаляя присут-ствующие на рисунке закоменченные строки !
Соблюдая расстановку пробелов ! Заголовки первого порядка (Нажимаем один TAB или 2 про-бела) , 
Второго порядка (2 TAB или 4 пробела), Третьего порядка (3 TAB или 6 пробелов).
version: '3'
services:
  db:
    image: mysql
    environment:
	MYSQL_DATABASE: mediawiki
	MYSQL_USER: wiki
	MYSQL_PASSWORD: DEP@ssw0rd
	MYSQL_ROOT_PASSWORD: DEP@ssw0rd
    ports:
      - 3306:3306
    volume:
      - /home/admin/dbvolume

  wiki:
    image: mediawiki
    ports:
      - 8080:80
#    volumes:
#     - /home/admin/LocalSettings.php:/var/www/html/LocalSettings.php

После чего запускаем контейнеры посредством команды
docker-compose -f /home/admin/wiki.yml up
После чего начнётся загрузка служб, после загрузки необходимо до-ждаться запуска контейнеров с сообщением 
о готовности подключения
Далее необходимо перейти на машину CLI , и в браузере перейти по ад-ресу  192.168.2.2:8080
Перейди по ссылке необходимо нажать → Continue 
Затем внизу страницы снова → Continue
Далее на следующей странице необходимо указать настройки по зада-нию , как указано на рисунке ниже. Пароль DEP@ssw0rd
На следующей странице нажимаем →  Continue
Далее на следующей странице, заполняем поля как указано на рисунке ниже, обязательно не забыв поставить галочку о том что вы очень занятой. Пароль DEP@ssw0rd
После чего сконфигурированный файл автоматически будет скачен в за-грузки
Далее его необходимо перенести на сервер. Если вы выполнили задание с запретом доступа по SSH с машины CLI, файл необходимо будет кидать не напрямую а через промежуточную машину HQ-R
Для этого воспользовавшись командами
На машине CLI от юзера админ (У вас может быть другой пользо-ватель в зависимости от кого вы авторизировались в систему):
scp /home/admin/Downloads/LocalSettings.php root@192.168.1.1:/home/admin
На машине HQ-R:
scp /home/admin/LocalSettings.php admin@192.168.1.2:/home/admin/
После чего необходимо на машине HQ-SRV перейти по пути
nano /home/admin/wiki.yml
И раскоментить и переписать (если они у вас отличаются)
После чего снова запустить контейнеры.
И теперь перейдя на машину CLI и зайдя в браузере по тому же адресу. Должна загрузится главная страница MediaWiki

