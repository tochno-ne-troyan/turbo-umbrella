Если не уверены, что справитесь с первого раза, то нужно сделать бэкапы, заходим в VMware под своим адресом, 
нажимаете на машину и сделать снимок состояния. 
Теперь при любом косяке можно вернутся в начало либо делать снимки при достижении какого-либо этапа.

Теперь пока все не испорчено или правильно сделано, скачаем все что нужно для выполнения заданий в дальнейшем:
Apt install frr(для frr, на всех роутерах(HQ, BR, ISP)
Apt install isc-dhcp-server(для dhcp, только на HQ)
Apt install radvd(настройка маршрутизации)
Apt install iperf3(для измерения пропускной способности на 2х маршрутизаторах HQ и ISP)
Apt install bind9 dnsutils(пакеты для днс будет стоять на сервере)(не надо)
apt install iptables-persistent(HQ-SRV для ssh)

Если закачка не идет то нужно вводить эти команды пока не заработает:
Dhclient -r
Dhclient -v

Так же чтобы зработать сразу больше балов можно создать учётки сразу

Учетки
adduser admin/branch/network
P@ssword
Admin/Branch admin/Network admin
Так же возможно понадобится выдать Root права для данных клиентов это можно выполнить посредством команды visudo
admin   ALL=(ALL:ALL) ALL (CLI HQ-SRV HQ-R)
branch  ALL=(ALL:ALL) ALL (BR-SRV BR-R)
network ALL=(ALL:ALL) ALL (HQ-R BR-R BR-SRV)

ПЕРВЫЙ МОДУЛЬ

Далее заходим на каждую машину и переименовываем их, там же вводим адрес в соответствии с топологией.
Все можно ввести, введя команду nmtui, если его нет вводим команду apt install network-manager,
в Set system hostname вводим имя, в Edit a connection вводим адреса, и вторая вкладка перезапускаем чтобы настройки принялись.

255.255.255.0 /24 маска — 1 сеть в которой 256 адресов от 0 до 255
255.255.255.128 /25 маска — 2 сети в каждой из которых по 128 адресов от 0 до 127 и от 128 до 256
255.255.255.192 /26 маска — 4 сети в каждой из которых по 64 адреса
255.255.255.224 /27 маска — 8 сетей по 32 адреса
255.255.255.240 /28 маска — 16 сетей по 16 адресов
255.255.255. 248 /29 маска — 32 сети по 8 адресов
255.255.255.252 /30 маска — 64 сети по 4 адреса
255.255.255.254 /31 маска — 128 сетей по 2 адреса
255.255.255.255 /32 маска — 256 сетей по 1 адресу

HQ-R
192.168.1.1/27 (МБ 28) - HQ-SRV
2001::1:1/123 (МБ 124) - HQ-SRV
10.10.10.1/30 - ISP
2001::7:1/126 - ISP
HQ-SRV
192.168.1.2/27 (МБ 28) - HQ-R
192.168.1.1 - gateway
2001::1:2/123 (МБ 124) - HQ-R
2001::1:1 - gateway
BR-R
192.168.2.1/29 - BR-SRV
2001::2:1/125 - BR-SRV
10.10.10.5/30 - ISP
2001::10:5/126 - ISP
BR-SRV
192.168.2.2/29 - BR-R
192.168.2.1 - gateway
2001::2:2/125 - BR-R
2001::2:2 - gateway
ISP
10.10.10.2/30 - HQ-R
2001::7:2/126 - HQ-R
192.168.0.1/24 - CLI
2001::3:1/120 - CLI
10.10.10.6/30 - BR-R
2001::7:6/126 - BR-R
CLI
192.168.0.2/24 - ISP
192.168.0.1 - gateway
2001::3:2/120 - ISP
2001::3:1 - gateway

После ввода имени нужно прописать команду newgrp это нужно для обновления, чтобы имя встало сразу.

Настройка FRR (BRR HQR ISP)
Nano /etc/frr/daemons
ospfd=yes
ospf6d=yes
Systemctl restart frr
Vtysh
Conf t
Router ospf
Ospf router-id 1.1.1.1|2.2.2.2|3.3.3.3
HQ – Network 10.10.10.0/30 area 0  
HQ – Network 192.168.1.0/27(28) area 1 
ISP – network 10.10.10.4/30 area 0 
ISP – network 192.168.0.0/24 area 2 
ISP – network 10.10.10.0/30 area 0 
BR – network 10.10.10.4/30 area 0
BR – network 192.168.2.0/29 area 3
Ex
Ex
Write
Ex
Nano /etc/sysctl.conf
переменную net.ipv4.ip_forward=1 необходимо раскоментить и сохранить изменения в файле, 
и применить изменения командой sysctl -p
Так же и с 6 net.ipv6.conf.all.forwarding=1
 
Router ospf6
Ospf6 router-id 0.0.0.1|0.0.0.2|0.0.0.3
HQ – Area 0.0.0.0 range 2001::7:0/126
HQ – Area 0.0.0.0 range 2001::1:0/123(124)
ISP – area 0.0.0.0 range 2001::7:4/126
ISP – area 0.0.0.0 range 2001::3:0/120
ISP – area 0.0.0.0 range 2001::7:0/126
BR – area 0.0.0.0 range 2001::7:4/126
BR – area 0.0.0.0 range 2001::2:0/125
Ex
Ex
Write
Ex

Нужно еще сделать один момент для работы ipv6:
Прописываем interface ens224*
	ipv6 ospf6 area 0.0.0.0
	ex
	!
	interface ens256*
И так на каждый интерфейс.
Применить изменения командой sysctl -p

Настройка DHCP - HQ-R
nano /etc/default/isc-dhcp-server

Нужно настроить интерфейс, направленный в сторону клиента, если в сети подразумевается DHCP-relay ,то
1 интерфейс в сторону клиента, 2 интерфейс в сторону запроса

Interfacesv4="ens* ens*"
Interfacesv6="ens* ens*"

Так же необходимо раскоментировать команды

nano /etc/dhcp/dhcpd.conf

default-lease-time 600;
max-lease-time 7200;
#ddns-updates on;
#ddns-update-style interim;
autoritative;

subnet 192.168.1.0 netmask 255.255.255.224(248) {
 range 192.168.1.3 192.168.1.30(14);
 option routers 192.168.1.1;
# option domain-name "hq.work";
# option domain-name-servers 192.168.1.2;
}

Где:
ddns-update-style interim — способ автообновления базы dns
authoritative — делает сервер доверенным
subnet — указание сети
range — пул адресов определяется количиство узлов маски -2 option routers — шлюз по умолчанию
subnet нужно 3 шт

nano /etc/default/isc-dhcp-server
раскомментить все v4 и если по задание необходимо и v6
изменить так же нужно: DHCPv4_PID=/var/run/dhclient.ens192.pid(192 если этот int интернет)

subnet 192.168.2.0 netmask 255.255.255.248 {
 range 192.168.2.3 192.168.2.6;
 option routers 192.168.2.1;
# option domain-name "hq.work";
# option domain-name-servers 192.168.2.2;
}

После каждого изменения конфигурации необходимо перезагружать DHCP сервер для применения конфигурации
systemctl stop isc-dhcp-server
systemctl start isc-dhcp-server
А для того, чтобы после перезагрузки DHCP-сервер автоматически включался можно воспользоваться командой 
systemctl enable isc-dhcp-server

Настройка v6

nano /etc/dhcp/dhcpd6.conf
default-lease-time 2592000;
preferred-lifetime 604800;
option dhcp-renewal-time 3600;
option dhcp-rebinding-time 7200;
allow leasequery;

subnet6 2001::1:0/123(124) {
	range6 2001::1:3 2001::2:3e;
option dhcp6.name-servers 2001::1:2;
option dhcp6.domain-search "hq.work";
}
option dhcp6.info-refresh-time 21600;
autoritative;

Однако dhcp6 не способен выдавать шлюз по умолчанию, эту функцию должен выполнять маршрутизатор
Поэтому для настройки маршрутизации для клиентов можно воспользоваться утилитой radvd

RADVD

nano /etc/radvd.conf 
следующего содержания
interface ens* (в сторону сервера, клиента)
{
MinRtrAdvInterval 3;
MaxRtrAdvInterval 60;
AdvSendAdvert on;
};

Где:
interface — это имя интерфейса направленного в локальную сеть
Min и MAX интервалы — это интервалы рассылки объявлений
AdvSendAdvert — это разрешение на выдачу объявлений от маршрутизатор клиентам

После окончания конфигурирования так же необходимо перезагрузить службу Radvd и отправить в Enable
systemctl stop radvd
systemctl start radvd
systemctl enable radvd
 
IPERF (HQ-R ISP)
После установки на обоих машинах, достаточно воспользоваться командной
iperf3 -c (ip адрес проверяемой машины) -i1 -t20

BACKUP - HQ-R BR-R
Для начала на машинах HQ-R, BR-R создадим каталог, где будет хранится файл созданного скриптом бекапа.
Можно создать его в директории mnt
для этого пропишем mkdir /mnt/backup
Далее нам нужно создать сам файл для создания бэкап скрипта, для этого пропишем команду
touch /etc/backup.sh

Если не зашёл автоматически пишем
nano /etc/backup.sh
В файле пишем:

#!/bin/bash
backup_files="/home /etc /root /boot /opt"
dest="/mnt/backup"
arhive_file="backup.tgz"
echo "Backing up $backup_files to $dest/$archive_file"
tar czf $dest/$archive_file $backup_files
echo "backup finished"
LS -lh $dest

где backup_files — копируемые директории
dest — место куда копируем директории
day — параметр который указывает день бэкапа
hostname — имя от кого он выполнился
archive_file — конечное имя файла
tar czf — в месте указанное в dest помещает файл с именем указанным в archive_file с содержимым указанным
в backup_files
echo — необязательные строки вывода


Для запуска скрипта достаточно написать bash (имя_файла)

После создания скрипта для того, чтобы распаковать наш backup архив можно воспользоваться командой
tar -xvpzf /mnt/backup/backup.tgz -C / --numeric-owner

Для того что бы не писать скрипт дважды, можно c помощью ssh перекинуть его на вторую машину посредством команды scp 
для начала подключаемся по ssh командой ssh имя@адрес

Пример: ssh network_admin@192.168.1.1

Затем посредством команды 
scp /расположение/имя_файла имя@адрес :/расположение/имя_файла
Пример:
 scp /etc/backup.sh network_admin@192.168.2.1:/home/network_admin

После успешного копирования возвращаемся в нашу машину и можем перенести скрипт в любое более удобное место
После всего нужно запустить скрипт: bash (имя_файла) 
		
SSH - HQ-SRV
nano /etc/ssh/sshd_config
Изменяем порт на 3035
systemctl restart ssh
правило iptables для подмены порта ssh:
iptables -t nat -A PREROUTING -d 192.168.1.0/27(28) -p tcp -m tcp --dport -destination 192.168.1.2:3035
(это одна команда)
iptables-save > /etc/iptables/rules.v4 

Контроль доступа

В зависимости от учётной записи, которая должна иметь доступ до сервера возможны следующие развития события, 
если нам необходим доступ только от локальных учётных записей,
то шаг 1 после всех настроек необходимо вернуть в исходный вид
Шаг 1 
Заходим в настройки ssh по пути использованному ранее
nano /etc/ssh/sshd_config
находим и меняем строку
 PermitRootLogin yes 

после сохранения изменений перезагружаем службу ssh
Шаг 2
Следующим шагом необходимо создать ключ аутентификации ssh  с помощью команды 
ssh-keygen -С «имя_устройства_с_которого_создан_ключ» везде необходимо нажать ENTER пока не создастся ключ 
Теперь необходимо перенести публичный ключ на сервер к которому мы будем получать доступ с помощью команды 
ssh-copy-id имя@адрес
Пример: 
ssh-copy-id root@192.168.1.2
	ssh-copy-id admin@192.168.1.2
	Последним шагом запретим любой доступ клиенту до нашего сервера
На HQ-SRV переходим по пути
nano /etc/hosts.deny
и вносим следующую строку в файл
sshd: 192.168.0.2 (адрес машины CLI)
перезагружаем ssh
В конце не забудьте отключить доступ по root, если иного не указано в задании !
nano /etc/ssh/sshd_config
PermitRootLogin no
systemctl restart sshd

ВТОРОЙ МОДУЛЬ

ЕСЛИ ЧТО СМОТРЕТЬ README

Вся настройка будет происходить на сервере HQ-SRV
Первым делом необходимо установить пакеты для dns командой
apt install bind9 dnsutils(в начале можно было установить или уже сделали)
Следующим шагом необходимо создать зоны для прямого и обратного просмотра dns
Для этого переходим по пути nano /etc/bind/named.conf.default-zones и создаём зоны как показано ниже
zone "hq.work" {  
  type master;  
  file "/etc/bind/hq";  
  allow-update {any;};  
  allow-transfer {any;};  
};

zone "1.168.192.in-addr.arpa" {  
  type master;  
  file "/etc/bind/hq_arpa";  
  allow-update {any;};  
};
zone "0.0.0.1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.2.ip6.arpa" {  
  type master;  
  file "/etc/bind/hq6_arpa";  
  allow-update {any;};  
};  

где:
zone — создаваемая зона
type — выбор между первичным и вторичным dns. (Master и Slave)
file — расположение конфигурационного файла зоны
allow-update — разрешение динамических обновлений
где zone:
hq.work — зона прямого просмотра
in-addr.arpa — зона обратного просмотра ipv4
ip6.arpa — зона обратного просмотра ipv6 (указывается полностью. В обратном порядке)
branch.work
zone "branch.work" {  
  type master;  
  file "/etc/bind/branch";  
  allow-update {any;};  
  allow-transfer {any;};  
};  
  
zone "2.168.192.in-addr.arpa" {  
  type master;  
  file "/etc/bind/branch_arpa";  
  allow-update {any;};  
};  
zone "0.0.0.2.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.2.ip6.arpa" {  
  type master;  
  file "/etc/bind/branch6_arpa";  
  allow-update {any;};  
};  

Следующим шагом необходимо создать конфигурационные файлы для наших зон. Это можно сделать,
скопировав стандартные шаблоны командой cp
Пример:
cp /etc/bind/db.local /etc/bind/hq — создание файла для прямой зоны
cp /etc/bind/db.127 /etc/bind/hq_arpa — создание обратной зоны ipv4
Зону для ipv6 скопируем после конфигурации зоны для ipv4 (так как по содержанию они не отличаются)
Первым шагом сконфигурируем зону прямого просмотра, переходим по пути
nano /etc/bind/hq 
И там заполняем
;
; BIND data file for local loopback interface
;
$TTL     604800
@        IN      SOA      hq.work.   root.hq.work. (
                                2          ;Serial
                            604800         ;Refresh
                             86400          ;Retry
                           2419200          ;Expire
                            604800 )        ;Negative Cache TTL
;
@        IN      NS       hq.work.      
@        IN      A        192.168.1.2
HQ-SRV   IN      AAAA     2001::1:2
HQ-SRV   IN      A        192.168.1.2
HQ-R     IN      A        192.168.1.1
HQ-R     IN      AAAA     2001::1:1
SERVER   IN      CNAME    HQ-SRV

Где:
NS запись — обозначение сервера отвественного за разрешение запросов к dns
A запись — основная запись для зоны прямого просмотра по протоколу ipv4
АААА запись - запись для зоны прямого просмотра по протоколу ipv6
CNAME — необязательный параметр, для указания альтернативного имени записи

Вторым шагом настроим зону обратного просмотра как указано ниже
Зона находится по пути 
nano /etc/bind/hq_arpa
И там заполняем
;
; BIND reverse data file for local loopback interface
;
$TTL     604800
@        IN      SOA      hq.work.   root.hq.work. (
                                 1          ;Serial
                            604800          ;Refresh
                             86400          ;Retry
                           2419200          ;Expire
                            604800 )        ;Negative Cache TTL
;
@        IN      NS       hq.work.      
2        IN      PTR      HQ-SRV.hq.work.
1        IN      PTR      HQ-R.hq.work.

Где:
PTR запись — основная запись для зоны обратного просмотра

Третьим шагом настроим запись для зоны обратного просмотра для ipv6, для этого достаточно скопировать зону hq_arpa, 
то есть
cp /etc/bind/hq_arpa /etc/bind/hq6_arpa
После создания всех конфигов необходимо перезагрузить службу bind9
systemctl restart bind9 (лучше stop и start)
Похожая настройка выполняется для зоны branch.work
Проверка выполняется посредством команд
host IP-адрес
host имя машины
Примечание:
Не забывайте, что для br-srv по заданию нет PTR записи, её создание может считаться ошибкой

NTP

Настройка производится на всех машинах, указанных в топологии, при этом настройка на машине, 
выступающей в роли NTP сервера уникальна, а на NTP клиентах идентична
Для начала на всех машинах необходимо установить московский часовой пояс, для этого следует воспользоваться командой 
timedatectl set-timezone Europe/Moscow
Следующим шагом установим альтернативную службу NTP, под названием CHRONY, так как для задания 3, где происходит развёртывание домена, будет использоваться именно этот сервис. Устанавливаем с помощью команды:
apt install chrony
Произведём установку NTP сервиса Chrony
Далее следует осуществить настройку машины, выступающей в роли NTP сервера HQ-R, посредством команды
nano /etc/chrony/chrony.conf
осуществим вход в конфигурацию chrony, где следует установить значения:
local stratum 5

allow 192.168.0.0/8  
allow 10.10.10.0/8 
 
Примечание: Нет необходимости указывать все сети которые присутствует в нашей сети, достаточно указать только одну сеть каждой машины , а так как у нас используется сети 192.168.0.0 , 192.168.1.0 и 192.168.2.0 , есть возможность взять сеть 192.168.0.0 с 22 маской которая будет включать в себя сеть начинающуюся с адреса 192.168.0.0 и заканчивающаяся адресом 192.168.3.255
Для настройки NTP клиентов chrony так же необходимо перейти в конфиг
nano /etc/chrony/chrony.conf
И необходимо провести внести изменения в конфиг

#Use Debian vendor zone.  
#pool 2.debian.pool.ntp.org iburst  
server 192. 168.1.1  
Для проверки используйте команды chronyc tracking и chronyc sources

Сервер домена. HQ-SRV
ДЛЯ настройки будет выбрана именно FreeIpa.
Первым делом необходимо установить докер, воспользовавшись скриптом, который есть в открытом доступе, 
однако для этого нам необходимо экспортировать переменные окружения относящиеся к Proxy 
(Если Proxy отсутствует т. е. Пакеты с не стандартных репозиториев устанавливаются сами,
то первый шаг можно пропустить)

ПЕРВЫМ ШАГОМ необходимо посмотреть переменные, которые необходимо экспортировать, перейдя по пути
nano /etc/apt/apt.conf.d/01proxy
и посмотреть находящиеся там значения, после чего посредством команд
export http_proxy=http(или https)://(адрес:порт)
export https_proxy=http(или https)://(адрес:порт)
 Экспортировать переменные прокси для доступа в интернет 
Acquire::http::Proxy "http://10.0.70.52:3128";
Acquire::https::Proxy "http://10.0.70.52:3128";
export http_proxy=http://10.0.70.52:3128
export https_proxy=http://10.0.70.52:3128

ВТОРЫМ ШАГОМ посредством скрипта необходимо установить сам DOCKER, для этого необходимо ввести следующую команду
wget -qO- https://get.docker.com | bash 
Вся установка происходит автоматически, и не должна выдавать ошибок, если были выполнены все предыдущие шаги

ТРЕТЬИМ ШАГОМ необходимо запулить готовый контейнер с образом freeipa для centos-8-4.8.4 
Для этого создаём каталог для автоматического запуска служб докера (Необходимо если вы делали шаги с Proxy ранее),
командой
mkdir -p /etc/systemd/system/docker.service.d
Далее заходим в файл 
nano /etc/systemd/system/docker.service.d/http-proxy.conf 
[Service]
Environment="HTTP_PROXY=http://10.0.70.52:3128"
Environment="HTTPS_PROXY=http://10.0.70.52:3128"
После чего перезапускаем демона и сам докер командами в указанном порядке
systemctl daemon-reload
и
systemctl restart docker
После чего запускаем команду
docker pull freeipa/freeipa-server:centos-8-4.8.4
После окончания пула контейнера необходимо создать директорию,
 в которую будет монтироваться контейнер посредством команды
mkdir -p /var/lib/ipa-data
 Также необходимо внести изменения в загрузчик системы для указания, 
необходимости использования обоих версий cgroup (механизм по ограничению ресурсов,
начиная с 11 Debian по умолчанию включена только 2 версия)
Для этого посредством команды заходим в загрузчик ядра
nano /etc/default/grub 
GRUB_CMDLINE_LINUX="quiet systemd.unified_cgroup_hierarchy=0"
Для применения изменений необходимо использовать команду
grub-mkconfig -o /boot/grub/grub.cfg
После чего необходимо перезагрузить машину
Следующим шагом уже переходим к запуску контейнера с хранящейся там FreeIPA,
в качестве параметров ключей, указывает имя, указываем доменную сеть,
а так открываем все необходимые для работы порты, указываем путь и образ, разрешаем конфликт с IPv6.
docker run --name freeipa-server -ti -h hq-srv.work -p 80:80 -p 443:443 -p 389:389 -p 636:636 -p 88:88 -p 464:464
-p 88:88/udp -p 464:464/udp -p 123:123/udp --read-only --sysctl net.ipv6.conf.all.disable_ipv6=0 -v 
/sys/fs/cgroup:/sys/fs/cgroup:rw -v /var/lib/ipa-data:/data:Z freeipa/freeipa-server:centos-8-4.8.4
Важное Примечание: В случае завершения выполняемых функций в контейнере в результате которых оболочка может перейти в состояние freezing, или при успешном завершении, для выхода из оболочки окружения необходимо последовательно нажать сочетание клавиш ctrl + p, а затем ctrl + q. В случае если вам необходимо остановить контейнер можно воспользоваться командой docker stop имя контейнера, для удаления контейнера docker rm имя контейнера, для просмотра существующих образов docker images 
После успешного запуска необходимо заполнить форму:
На вопрос о интеграции DNS нажимаем Enter
На вопрос о задании имени сервера нажимаем Enter
На вопрос о подтверждение имени домена нажимаем Enter
На вопрос о подтверждение имени области нажимаем Enter
На запрос ввода пароля для менеджера директорий вводим P@ssw0rd
На запрос ввода пароля для IPA админа вводим P@ssw0rd
На вопрос синхронизации с службой Chrony нажимаем Enter
На вопрос о конфигурирование системы с текущими параметрами вводим yes
Процесс установки достаточно длительный и может занимать около 5-10 или более минут.
После завершения установки необходимо подготовить машины, которые будут присоединены к домену. Для этого первым делом переходим по пути:
Nano /etc/hosts
127.0.0.1	localhost
127.0.1.1	cli.hq.work	cli

192.168.1.2 hq-srv.hq.work (hq.srv.serv)

Для машины BR-SRV настройка будет выглядеть 
127.0.0.1	localhost
127.0.1.1	br-srv.branch.work	br-srv

192.168.1.2 hq-srv.hq.work (hq.srv.serv)

Следующим шагом посредством команды:
apt install freeipa-client
Производим установку клиентской части FreeIPA для ввода машины в домен.
На все всплывающие окна во время установки нажимаем Enter
После установки клиента, для ввода машины в домен необходимо прописать команды: 
НА CLI
ipa-client-install --mkhomedir --domain hq.work --server=hq-srv.hq.work -p admin -W
НА BR-SRV
ipa-client-install --mkhomedir --domain branch.work --server=hq-srv.hq.work -p admin -W

На сообщение о продолжении с фиксированными значения пишем yes
На вопрос о конфигурирование CHRONY нажимаем ENTER
На вопрос о конфигурировании с текущими значение пишем yes
Для проверки входа в FreeIPA, на клиентской машине необходимо открыть браузер и в адресной строке написать IP адрес 
машины HQ-SRV (192.168.1.2) логин и пароль для входа в вебку FreeIPA: admin и P@ssw0rd
Важное Примечание: если вы перезагрузите машину, то контейнер выключится, для его запуска можно воспользоваться 
командой  docker start freeipa-server

SMB или NFS - HQ-SRV
Исходя из поставленной задачи NFS будет более удачным выбором из-за его большей совместимости с системами Linux, 
при этом SMB крайне пере-гружена за счёт того, что создан для совместного использования широкого спектра сетевых 
ресурсов, включая службы файлов и печати, устройства хра-нения данных и хранилища виртуальных машин, в время как NFS, 
для сов-местного использования файлов и каталогов.
Поскольку файловый сервер будет работать на основе NFS , первым де-лом необходимо установить NFS сервер , 
посредством команды:
apt install nfs-kernel-server
Далее необходимо создать каталоги которые будут расшариваться.
mkdir /mnt/all — создание корневого каталога в котором будут хранит-ся остальные
mkdir /mnt/all/Branch_Files — каталог для пользователя branch_admin
mkdir /mnt/all/Network — каталог для пользователя network_admin
mkdir /mnt/all/Admin_Files — каталог для пользователя admin
Так же для того что бы монтируемые директории не были пустыми , и был виден результат монтирования посредством команд
touch /mnt/all/Branch_Files/123
touch /mnt/all/Network/234
touch /mnt/all/Admin_Files/345
Создадим файлы с разными имена в директориях
Далее посредством команды 
nano /etc/exports 
Заходим в конфигурационный файл , где будут прописываться все об-щие ресурсы и их параметры и заполняем
/mnt/all/Branch_Files *(rw,async,no_subtree_check)
/mnt/all/Network_Files *(rw,async,no_subtree_check)
/mnt/all/Admin_Files *(rw,async,no_subtree_check)
где:
 /mnt/all/имя — Указание директории на сервере до которой будет вы-дан общий доступ
* - указание IP адресов, которые имеют доступ в эту директорию (звёз-дочка значит все, 
так как по заданию не указано делать ограничения)
rw — разрешение на чтение и запись
async — включение обработки запросов клиента , до окончания преды-дущего действия
no_subtree_check — отключает проверку вложенных директорий
Для экспорта всех общих ресурсов необходимо воспользоваться коман-дой
exportfs -ra 
Также ещё одним необходимым шагом является создание доменных пользователей в Freeipa домене,
для этого посредством адреса необходимо зайти  в web-интерфейс Freeipa (адрес 192.168.1.2) , 
и сконфигурировать всех пользователей которые необходимы по заданию
Важное примечание: Пользователь admin является встроенной учётной записью и его конфигурировать не нужно.
Во вкладке users необходимо нажать кнопку add
И в появившемся окне необходимо заполнить следующие данные:
user login — network_admin или branch_admin
first name — Network Admin или Branch Admin
last name - Network Admin или Branch Admin
New Password - 123
Verify Password -123
Пароль задаётся 123 , поскольку после захода в систему, необходимо будет сменить пароль
После ввода параметров необходимо нажать Add and Edit
и сконфигурировать параметры Login shell и Home directory. Пример конфигурирования для пользователя branch_admin
где:
Login shell — изменения оболочки окружения в которую будем попа-дать при входе с sh(shell) на bash
Home directory — изменение домашней директории , необходимо так как иначе она будет совпадать с директориями локальных пользователей со-зданных на машинах
После этого можно перейти к настройке клиента , т. к. в задании указано что монтирование должно осуществляться 
при входе доменного пользователя , настройка будет проводится на машинах которые занесены в домен CLI и BR-SRV
Первым шагом необходимо установить NFS-клиент и Pam модуль для автоматического монтирования разделов при входе 
пользователя командой:
apt install nfs-common libpam-mount
Так же необходимо создать каталог куда будет проводится монтирова-ние
mkdir /mnt/all
После чего перейдя по пути
nano /etc/security/pam_mount.conf.xml
Необходимо добавить строки приведённые ниже в разделе <volume definitions>
<volume user="admin" fstype="nfs" server="192.168.1.2" path="/mnt/all/Admin_Files" mountpoint="/mnt/all" />
<volume user="branch_admin" fstype="nfs" server="192.168.1.2" path="/mnt/all/Branch_Files" mountpoint="/mnt/all" />
<volume user="network" fstype="nfs" server="192.168.1.2" path="/mnt/all/Network" mountpoint="/mnt/all" />
Для проверки работы общих ресурсов необходимо зайти под доменным пользователем, для этого посредством команды 
sudo login
переходим в окно для входа в систему
в владке Login указывается пользователь по шаблону :
имя@домен
Пример:
branch_admin@hq.work
В вкладке Password вводится пароль, для пользователей branch_admin и network_admin необходимо будет ввести пароль по 
схеме:
Password: 123
Current Password: 123
New password: P@ssw0rd
Retype new password: P@ssw0rd
И зайдя в пользователя проверить содержимое папки /mnt/all на наличие созданных файлов

веб-сервер LMS.
Вся настройка пунктов A и B будет выполнятся исключительно на ма-шине BR-SRV, для пункта C,
а также проверки пункта A необходимо вос-пользоваться машиной CLI, так как на ней присутствует графика.
Первым шагом необходимо установить пакеты для веб-сервера APACHE и пакеты поддержки PHP, так как PHP, 
быстрее всего позволит со-здать страницу с номер места сдающего.
Для этого посредством команды
Apt install apache2 libapache2-mod-php
Устанавливаются пакеты для apache сервера и поддержки PHP сервером
Далее необходимо сконфигурировать страницу, которой в будущем за-менится дефолтная страница APACHE, командой
nano /var/www/html/mesto.php
Создаётся страница, которую необходимо заполнить
<?php
$fontSize = "200px";
echo "<div style=\"text-align:center\">";
print '<p style="font-size:' .htmlspecialchars($fontSize). '; ">5/p>';
?>

Единственная часть  кода, которую необходимо будет менять, это цифра 5 ,
её будет необходимо заменить на номер своего места.

Следующим шагом необходимо заменить дефолтную страницу, 
для того что бы при обращение к серверу в качестве главной страницы, показывался номер места,
для этого перейдя по пути
nano /etc/apache2/apache2.conf
Переходим в конфигурационный файл, и ищем и заполняем раздел
 <Directory /var/www/>
	options Indexes FollowSymLinks
	AllowOverride None
	Require all granted
	DirectoryIndex mesto.php
</Directory>
Для проверки достаточно зайти на машину CLI, и в браузере прописать IP-адрес сервера, если ошибок допущено не было,
должна быть выведена цифра по центру.
Далее переходим к установке базы данных , т. к. напрямую mysql-server установить не получится,
будет использоваться пользовательский пакет, необ-ходимо снова прописать команду для экспорта переменных которая была 
в  задание 3.
Далее установим один из пакетов необходимых для работы mysql ко-мандой
apt install gnupg
Далее посредством команды
wget https://dev.mysql.com/get/mysql-apt-config_0.8.29-1_all.deb
Указывается путь откуда будет скачиваться пакет, после чего для уста-новки не user friendly пакетов,
используется команда
dpkg -i ./mysql-apt-config_0.8.29-1_all.deb
После чего при установке в появившемся окне просто выбирается вариант OK
Далее для обновления репозитория mysql необходимо прописать:
apt update 
После успешного обновления, можно переходить к установке пакетов для mysql , для этого командой
apt install mysql-server php-mysql
Устанавливаются пакеты сервера, и его совместимости с php, второй из них пригодится чуть позже.
Во время установки будет предложено установить пароль, указывается пароль P@ssw0rd, на вопрос о плагине аутентификации 
выбирается первый вариант
Далее для создания пользователей и другим ,можно установить веб-интерфейс для субд mysql, под названием phpmyadmin, 
этот шаг не обязате-лен, если вы самостоятельно можете создать пользователей и группы че-рез консоль управления 
mysql-server.
Для установки phpmyadmin, необходимо воспользоваться командой:
apt install phpmyadmin
Во время установки:
На вопрос о выборе сервера для конфигурации нажимаем Space (Пробел) напротив Apache2, что бы появилась звёздочка.
После чего Enter.
На вопрос о конфигурирование БД для phpmyadmin выбирается вари-ант YES
Во всех вариантах где необходимо ввести пароль вводится пароль P@ssw0rd
Далее необходимо перейти по адресу
IP-адрес сервера/phpmyadmin
Пример
192.168.1.2/phpmyadmin
В окне авторизации:
В поле Username вводится root (регистр имеет значение)
В поле Password вводится P@ssw0rd
После успешной авторизации заходим в User account(add user account)
В открывшемся окне, заполняются поля(имя, пароль) и go снизу
После создания снова необходимо перейти в вкладку User accounts,
В открывшемся окне указывается имя группы которая будет создана.
Потом заходим там же в user account overview, edit user group.
И в открывшемся окне, указывается группа

Mediawiki.
Первым шагом необходимо установить docker compose, так как сам до-кер устанавливался в задании №3 второго модуля.
Для этого посредством команды и скачиваем:
curl -L "https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-$(uname -$)-$(uname -m) 
-o /usr/local/bin/docker-compose
Выдача прав: chmode +X /usr/local/bin/docker-compose
Далее для того, чтобы с нуля не писать yml файл, можно скачать похо-жий по смыслу файл, 
приведённый на рисунке ниже (если будет запрещено, будете писать сами)
wget -L "https://raw.githubusercontent.com/pirate/wikipedia-mirror/master/docker-compose.mediawiki.yml" 
-O /home/admin/wiki.yml
После чего открываем скачанный файл по пути
Nano /home/admin/wiki.yml
И приводим к виду, указанному на рисунке ниже, не удаляя присут-ствующие на рисунке закоменченные строки !
Соблюдая расстановку пробелов ! Заголовки первого порядка (Нажимаем один TAB или 2 про-бела) , 
Второго порядка (2 TAB или 4 пробела), Третьего порядка (3 TAB или 6 пробелов).
version: '3'
services:
  db:
    image: mysql
    environment:
	MYSQL_DATABASE: mediawiki
	MYSQL_USER: wiki
	MYSQL_PASSWORD: DEP@ssw0rd
	MYSQL_ROOT_PASSWORD: DEP@ssw0rd
    ports:
      - 3306:3306
    volume:
      - /home/admin/dbvolume

  wiki:
    image: mediawiki
    ports:
      - 8080:80
#    volumes:
#     - /home/admin/LocalSettings.php:/var/www/html/LocalSettings.php

После чего запускаем контейнеры посредством команды
docker-compose -f /home/admin/wiki.yml up
После чего начнётся загрузка служб, после загрузки необходимо до-ждаться запуска контейнеров с сообщением 
о готовности подключения
Далее необходимо перейти на машину CLI , и в браузере перейти по ад-ресу  192.168.1.2:8080
Перейди по ссылке необходимо нажать → Continue 
Затем внизу страницы снова → Continue
Далее на следующей странице необходимо указать настройки по зада-нию , как указано на рисунке ниже. Пароль DEP@ssw0rd
На следующей странице нажимаем →  Continue
Далее на следующей странице, заполняем поля как указано на рисунке ниже, обязательно не забыв поставить галочку о том что вы очень занятой. Пароль DEP@ssw0rd
После чего сконфигурированный файл автоматически будет скачен в за-грузки
Далее его необходимо перенести на сервер. Если вы выполнили задание с запретом доступа по SSH с машины CLI, файл необходимо будет кидать не напрямую а через промежуточную машину HQ-R
Для этого воспользовавшись командами
На машине CLI от юзера админ (У вас может быть другой пользо-ватель в зависимости от кого вы авторизировались в систему):
scp /home/admin/Downloads/LocalSettings.php root@192.168.1.1:/home/admin
На машине HQ-R:
scp /home/admin/LocalSettings.php admin@192.168.1.2:/home/admin/
После чего необходимо на машине HQ-SRV перейти по пути
nano /home/admin/wiki.yml
И раскоментить и переписать (если они у вас отличаются)
После чего снова запустить контейнеры.
И теперь перейдя на машину CLI и зайдя в браузере по тому же адресу. Должна загрузится главная страница MediaWiki



